{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Models with Strands Agents\n",
    "\n",
    "Strands Agents supports multiple model providers, giving you flexibility in choosing the right model for your use case. This notebook covers different model configurations and their trade-offs.\n",
    "\n",
    "## What you'll learn\n",
    "- Working with Amazon Bedrock models\n",
    "- Configuring local models with Ollama\n",
    "- Model selection criteria and performance considerations\n",
    "- Cost optimization strategies\n",
    "\n",
    "## Prerequisites\n",
    "- Completed notebooks 01-02\n",
    "- AWS credentials configured (for Bedrock)\n",
    "- Ollama installed (for local models - optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom models setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from strands import Agent\n",
    "from strands.models import BedrockModel\n",
    "from strands.models.ollama import OllamaModel\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.getLogger(\"strands\").setLevel(logging.WARNING)\n",
    "\n",
    "# Optional: Set MEM0 API key for memory features (get your free key from https://mem0.ai)\n",
    "MEM0_API_KEY = os.getenv(\"MEM0_API_KEY\")\n",
    "if MEM0_API_KEY:\n",
    "    os.environ[\"MEM0_API_KEY\"] = MEM0_API_KEY\n",
    "\n",
    "print(\"Custom models setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "Let's configure different models for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models configured successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configure Bedrock model\n",
    "bedrock_model = BedrockModel(\n",
    "    model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    region_name=\"us-west-2\",\n",
    "    temperature=0.3, # Randomness, (higher number = more random)\n",
    ")\n",
    "\n",
    "# Configure Ollama model (requires Ollama running locally)\n",
    "ollama_model = OllamaModel(\n",
    "    host=\"http://localhost:11434\",\n",
    "    model_id=\"llama3\",\n",
    "    temperature=0.3, # Randomness, (higher number = more random)\n",
    ")\n",
    "\n",
    "print(\"Models configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Agents with Custom Models\n",
    "\n",
    "Now let's create agents using our configured models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create agents with different models\n",
    "bedrock_agent = Agent(model=bedrock_model)\n",
    "ollama_agent = Agent(model=ollama_model)\n",
    "\n",
    "print(\"Agents created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare the performance of different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Question: Explain quantum computing in one sentence.\n",
      "\n",
      "‚ö° Bedrock Model (Sonnet 4):\n",
      "Quantum computing harnesses the strange properties of quantum mechanics‚Äîlike superposition (being in multiple states simultaneously) and entanglement‚Äîto process information in ways that could solve certain complex problems exponentially faster than classical computers.Time: 2.63s\n"
     ]
    }
   ],
   "source": [
    "question = \"Explain quantum computing in one sentence.\"\n",
    "print(f\"üí¨ Question: {question}\")\n",
    "\n",
    "# Test Bedrock model\n",
    "print(\"\\n‚ö° Bedrock Model (Sonnet 4):\")\n",
    "start = time.time()\n",
    "bedrock_response = bedrock_agent(question)\n",
    "bedrock_time = time.time() - start\n",
    "print(f\"Time: {bedrock_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Ollama model (llama 3):\n",
      "Quantum computing is a type of computing that uses the principles of quantum mechanics, such as superposition and entanglement, to perform calculations on data that exists in multiple states simultaneously, allowing for exponentially faster processing times and new possibilities for solving complex problems.Time: 6.41s\n",
      "\n",
      "üìä Speed difference: 2.4x\n"
     ]
    }
   ],
   "source": [
    "# Test Ollama model (only if Ollama is running)\n",
    "try:\n",
    "    print(\"\\nüß† Ollama model (llama 3):\")\n",
    "    start = time.time()\n",
    "    ollama_response = ollama_agent(question)\n",
    "    ollama_time = time.time() - start\n",
    "    print(f\"Time: {ollama_time:.2f}s\")\n",
    "    \n",
    "    print(f\"\\nüìä Speed difference: {ollama_time/bedrock_time:.1f}x\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Ollama not available: {e}\")\n",
    "    print(\"üí° To use Ollama: Install Ollama and run 'ollama pull llama3'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Guidelines\n",
    "\n",
    "### When to use Bedrock (Cloud):\n",
    "- **Production applications** requiring high reliability\n",
    "- **Complex reasoning tasks** needing advanced capabilities\n",
    "- **Scalable solutions** with automatic infrastructure management\n",
    "- **Enterprise compliance** requirements\n",
    "\n",
    "### When to use Ollama (Local):\n",
    "- **Privacy-sensitive** applications\n",
    "- **Offline environments** without internet access\n",
    "- **Development and testing** to reduce costs\n",
    "- **Custom model fine-tuning** requirements\n",
    "\n",
    "### Performance Considerations:\n",
    "- **Latency**: Local models typically faster for simple tasks\n",
    "- **Quality**: Cloud models generally more capable\n",
    "- **Cost**: Local models free after setup, cloud models pay-per-use\n",
    "- **Scalability**: Cloud models handle concurrent users better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Model Configuration\n",
    "\n",
    "You can fine-tune model behavior with additional parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Bedrock configuration\n",
    "advanced_bedrock = BedrockModel(\n",
    "    model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    region_name=\"us-west-2\",\n",
    "    temperature=0.7,  # More creative\n",
    "    max_tokens=1000,  # Longer responses\n",
    "    top_p=0.9,        # Nucleus sampling\n",
    ")\n",
    "\n",
    "# Create agent with advanced configuration\n",
    "creative_agent = Agent(\n",
    "    model=advanced_bedrock,\n",
    "    system_prompt=\"You are a creative assistant. Be imaginative and detailed.\"\n",
    ")\n",
    "\n",
    "# Test creative response\n",
    "creative_response = creative_agent(\"Write a creative story about AI in 3 sentences.\")\n",
    "print(\"üé® Creative Agent Response:\")\n",
    "print(creative_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned how to:\n",
    "\n",
    "‚úÖ **Configure custom models** with BedrockModel and OllamaModel  \n",
    "‚úÖ **Compare performance** between cloud and local models  \n",
    "‚úÖ **Optimize model parameters** for different use cases  \n",
    "‚úÖ **Choose the right model** based on requirements  \n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different temperature settings\n",
    "- Try other Ollama models (llama3.2:1b, llama3.2:3b)\n",
    "- Test with your specific use cases\n",
    "- Consider cost vs. performance trade-offs\n",
    "\n",
    "**Continue to notebook 04 to learn about memory-enabled agents! üß†**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
